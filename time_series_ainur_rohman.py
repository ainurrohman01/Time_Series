# -*- coding: utf-8 -*-
"""Submission_Time_Series_Ainur_Rohman.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vm25rtvNt1Y7UY2yfDVIigXgSbjyc_I-

Nama  : Ainur Rohman

Email : ainur.solver@gmail.com

# Data Collecting
"""

# Menginstall kaggle : Berfungsi memudahkan dalam mengorganisi package dataset dari kaggle
!pip install -q kaggle

# Code ini berfungsi untuk mengupload API kaggle.json
from google.colab import files
files.upload()

! mkdir -p ~/.kaggle              # Membuat direktori kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json # Untuk memberikan akses terbatas

! kaggle --version

# Mendownload dataset  "cuaca kota delhi"
!kaggle datasets download -d mahirkukreja/delhi-weather-data

# Membuat direktori bbc news
!mkdir delhi-weather-data

# Mengekstrak fil zip
!unzip delhi-weather-data.zip -d delhi-weather-data
!ls delhi-weather-data

"""# Data Preprocessing"""

# Import library yang digunakan

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM
import tensorflow as tf

"""## Membaca Dataset hasil unzip

## Processing menggunakan library pandas
"""

# Memuat dataset

data_weather = pd.read_csv('delhi-weather-data/testset.csv')

# Menampilkan 5 data teratas

data_weather.head()

# Menampilkan 5 data terbawah

data_weather.tail()

# Mengecek tipe data apa saja yang terdapat dalam dataset

data_weather.info()

"""## Menghitung jumlah/total banyaknya sample pada dataset"""

data_weather.info

"""**Dataset yang digunakan memiliki 100990 sample**"""

# Untuk mengecek data null
data_weather.isnull().sum()

data_weather['datetime_utc']=pd.to_datetime(data_weather['datetime_utc'])
data_weather['datetime_utc'].head()
data_weather[' _tempm'].fillna(data_weather[' _tempm'].mean(), inplace=True)
data_weather = data_weather[['datetime_utc',' _tempm' ]]
data_weather.head()

# mengecek informasi baru

data_weather.info()

"""Mengubah nama variabel"""

Date = data_weather['datetime_utc'].values
Temp  = data_weather[' _tempm'].values

"""Plotting Data"""

plt.figure(figsize=(15,5))
plt.plot(Date, Temp)
plt.title('Delhi Weather',
              fontsize=15);
plt.xlabel('Waktu')
plt.ylabel('Suhu')
plt.show()

"""# Membagi dataset sebanyak 20 %"""

x_train, x_test, y_train, y_test = train_test_split(Temp, Date, test_size = 0.2, random_state = 42)
print(len(x_train), len(x_test))

"""Setelah dilakukan splitting data, diperoleh:

Data train = 80792

Data test = 20198

Melakukan Preprocessing data dengan Min Max Scaller
"""

minimum = min(x_train)
maximum = max(x_train)
print(minimum,maximum)
mae = (maximum - minimum) / 10
print(mae)

"""## Mengubah dalam bentuk numpy"""

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

data_x_train = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=1000)
data_x_test = windowed_dataset(x_test, window_size=60, batch_size=100, shuffle_buffer=1000)

"""# Membuat Arsitektur dengan LSTM dan Menggunakan Model Sequential"""

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1)  
])

"""## Menggunakan Fungsi Callback

Callback Function berfungsi untuk memberi tahu model kita untuk berhenti ketika telah mencapai akurasi tertentu, sehingga proses pelatihan model menjadi lebih singkat dan untuk efisiensi waktu pelatihan.
"""

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
      if(logs.get('mae') < mae):
        print("\nMAE sudah < 10 % skala data")
        self.model.stop_training = True
callbacks = myCallback()

"""#Menggunakan optimizer"""

optimizer = tf.keras.optimizers.SGD(learning_rate=1.0000e-04, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),            # Loss function yang dapat dicoba untuk ini adalah Huber yang merupakan salah satu loss function yang umum digunakan pada kasus time series
              optimizer=optimizer,
              metrics=["mae"])                         # metrik evaluasi dengan mean absolute error (mae)

"""# Pelatihan"""

num_epochs=100
history = model.fit(data_x_train, callbacks=[callbacks], validation_data=data_x_test, epochs=num_epochs)

"""Waktu pelatihan adalah 2 menit 55 detik dan mae < 10 % skala data

# Membuat Plot Grafik Hasil
"""

plt.plot(history.history['mae'], 'g')
plt.plot(history.history['val_mae'], 'b')
plt.title('Plot/Grafik mae')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Mae Train','Mae Test'], loc='upper right')
plt.show()

plt.plot(history.history['loss'], 'y')
plt.plot(history.history['val_loss'], 'r')
plt.title('Plot/Grafik loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Loss Train', 'Loss Test'], loc='upper right')
plt.show()